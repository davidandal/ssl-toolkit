{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c752f67",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f9a83",
   "metadata": {},
   "source": [
    "Essential libraries for data handling, model training, and dataset preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed212ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb6285d",
   "metadata": {},
   "source": [
    "Imports the custom factory modules used in the pipeline:\n",
    "- `token_factory`: Handles preprocessing/tokenization based on input type  \n",
    "- `dataloader_factory`: Builds dataloaders for labeled, unlabeled, and validation sets  \n",
    "- `model_factory`: Constructs models based on input type and task\n",
    "\n",
    "The modules are reloaded using `importlib.reload()` to reflect any recent changes without restarting the environment (useful in interactive environments like notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecde0b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import token_factory as tf\n",
    "import dataloader_factory as dl\n",
    "import model_factory as md\n",
    "\n",
    "importlib.reload(tf)\n",
    "importlib.reload(dl)\n",
    "importlib.reload(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b1a70",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd897e",
   "metadata": {},
   "source": [
    "This dictionary contains all the settings used for training the pseudo-labeling model on tabular data.\n",
    "\n",
    "Sections:\n",
    "- **General**: Includes session ID and random seed for reproducibility.\n",
    "- **Pseudo-Labeling Settings**:\n",
    "  - `learning_rate`: Controls how fast the model learns.\n",
    "  - `confidence_threshold`: Minimum confidence required to accept a pseudo-label from the model.\n",
    "  - `epochs`: Number of full passes through the training data.\n",
    "- **Dataset Paths & Structure**:\n",
    "  - Dataset paths should point to appropriate folders or files for labeled, unlabeled, and validation data.\n",
    "  - `input_type`: Specify \"image\", \"text\", or \"tabular\".\n",
    "  - `validation_set_percentage`: Used to split part of the labeled data for validation.\n",
    "- **Tabular Input Columns**:\n",
    "  - `categorical_columns`: List of columns treated as categories (e.g., gender, city).\n",
    "  - `numeric_columns`: List of columns with continuous values (e.g., age, income).\n",
    "  - `tabular_target_column`: Column name of the prediction target (must be categorical for this setup).\n",
    "<br><br>\n",
    "> ðŸ’¡ Detailed explanations of configuration variables can be found in the README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ffd2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # General\n",
    "    \"training_session\": 1,\n",
    "    \"seed\": 27,\n",
    "\n",
    "    # Pseudo-Labeling Model\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"confidence_threshold\": 0.98,\n",
    "    \"epochs\": 20,\n",
    "\n",
    "    # Dataset\n",
    "    \"labeled_dataset_path\": \"\",\n",
    "    \"unlabeled_dataset_path\": \"\",\n",
    "    \"validation_set_percentage\": 0,\n",
    "    \"batch_size\": 64,\n",
    "\n",
    "    # Tabular input\n",
    "    \"categorical_columns\": [],\n",
    "    \"numeric_columns\": [],\n",
    "    \"tabular_target_column\": \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805be46b",
   "metadata": {},
   "source": [
    "### Training Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97be55e9",
   "metadata": {},
   "source": [
    "***train_one_epoch(...)***\n",
    "\n",
    "Trains the model for a single epoch using labeled data.\n",
    "\n",
    "How it works:\n",
    "- Sets the model to training mode (enables dropout, batchnorm, etc.).\n",
    "- Iterates through each batch of labeled data.\n",
    "- Computes the **Cross Entropy Loss**, suitable for classification tasks.\n",
    "- Performs backpropagation and updates the model using the optimizer.\n",
    "\n",
    "Parameters:\n",
    "- `model`: The classification model to train.\n",
    "- `loader`: Dataloader containing labeled training samples.\n",
    "- `device`: Target device (CPU or GPU).\n",
    "- `optimizer`: Optimizer used to update model weights.\n",
    "<br><br>\n",
    "> ðŸ’¡ This function is used during the initial supervised training phase and later when retraining with pseudo-labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c8117f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, device, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    total_loss = 0.00\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "\n",
    "        loss = loss_function(outputs, y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Total Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbddd72",
   "metadata": {},
   "source": [
    "***evaluate(...)***\n",
    "\n",
    "Evaluates the modelâ€™s performance on a labeled validation dataset.\n",
    "\n",
    "How it works:\n",
    "- Switches the model to **evaluation mode** (disables dropout, no gradient tracking).\n",
    "- Iterates over the validation data.\n",
    "- Computes the **cross-entropy loss** for classification tasks.\n",
    "- Generates predicted class labels using `argmax`.\n",
    "- Calculates and prints **accuracy** across all validation samples.\n",
    "\n",
    "Parameters:\n",
    "- `model`: The trained model to evaluate.\n",
    "- `loader`: Dataloader for the validation set.\n",
    "- `device`: Target device (CPU or GPU).\n",
    "<br><br>\n",
    "> ðŸ’¡ This function is used to monitor validation performance after training and pseudo-labeling phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c34fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions, all_labels = [], []\n",
    "    total_loss = 0.00\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "            loss = F.cross_entropy(logits, y.long())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f} | Loss: {total_loss:.4f}\")\n",
    "    \n",
    "    return all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b47b52",
   "metadata": {},
   "source": [
    "***train_without_pseudo_labels(...)***\n",
    "\n",
    "Performs the **initial training phase** of the Pseudo-Labeling algorithm using only labeled data. After training, the model is used to generate pseudo-labels for confident predictions on the unlabeled data.\n",
    "\n",
    "Phase 1: Supervised Training\n",
    "- Trains the model using only the labeled dataset.\n",
    "- After each epoch, the model is evaluated on a validation set.\n",
    "- The best-performing model (based on validation accuracy) is saved to disk.\n",
    "\n",
    "Phase 2: Pseudo-Label Generation\n",
    "- Switches the model to evaluation mode.\n",
    "- For each batch of **unlabeled data**:\n",
    "  - Predicts class probabilities using `softmax`.\n",
    "  - Retains only the samples where the model is confident (`confidence >= threshold`).\n",
    "  - These confident predictions are returned as pseudo-labeled data.\n",
    "\n",
    "Parameters:\n",
    "- `model`: The tabular classification model to train.\n",
    "- `labeled_loader`: Dataloader containing the labeled training data.\n",
    "- `validation_loader`: Dataloader for evaluating validation performance.\n",
    "- `unlabeled_loader`: Dataloader for the unlabeled dataset (no ground truth).\n",
    "- `device`: The target device (CPU or GPU).\n",
    "<br><br>\n",
    "> âš ï¸ Only confident predictions are retained to reduce the risk of propagating noisy pseudo-labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8cabda6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_without_pseudo_labels(model, labeled_loader, validation_loader, unlabeled_loader, device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    best_model_path = f\"../../models/pseudo_label/best_model_tabular_{config[\"training_session\"]}.pt\"\n",
    "    best_accuracy = 0.00\n",
    "\n",
    "    # Train on labeled data\n",
    "    for epoch in range(1, config[\"epochs\"] + 1):\n",
    "        print(f\"--- Start of Epoch {epoch}! ---\")\n",
    "\n",
    "        train_one_epoch(model, labeled_loader, device, optimizer)\n",
    "\n",
    "        predictions, labels = evaluate(model, validation_loader, device)\n",
    "        validation_accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "        if validation_accuracy > best_accuracy:\n",
    "            best_accuracy = validation_accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "    \n",
    "            print(f\"âœ… Best model saved to {best_model_path} | Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "    print(f\"--- End of Training ---\")\n",
    "    \n",
    "    # Generate pseudo-labels\n",
    "    model.eval()\n",
    "    pseudo_features, pseudo_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x in unlabeled_loader:\n",
    "            x = x.to(device)\n",
    "\n",
    "            outputs = model(x)\n",
    "\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            confidence, predictions = torch.max(probabilities, dim=1)\n",
    "            mask = confidence >= config[\"confidence_threshold\"]\n",
    "\n",
    "            pseudo_features.append(x[mask].cpu())\n",
    "            pseudo_labels.append(predictions[mask].cpu())\n",
    "            \n",
    "    return pseudo_features, pseudo_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926c561",
   "metadata": {},
   "source": [
    "***train_with_pseudo_labels(...)***\n",
    "\n",
    "Fine-tunes the model using a **combined dataset** of real labeled samples and high-confidence pseudo-labeled samples generated from the previous phase.\n",
    "\n",
    "How it works:\n",
    "- Continues training the model using the expanded dataset (original + pseudo-labeled).\n",
    "- Tracks validation accuracy after each epoch.\n",
    "- Saves the best-performing model to a separate path (`_pseudo.pt` suffix).\n",
    "\n",
    "Parameters:\n",
    "- `model`: The tabular model to be fine-tuned.\n",
    "- `labeled_loader`: Dataloader containing both real and pseudo-labeled training data.\n",
    "- `validation_loader`: Dataloader used to monitor model performance.\n",
    "- `device`: Target device (CPU or GPU).\n",
    "<br><br>\n",
    "> ðŸ’¡ This phase helps the model generalize better by exposing it to confidently predicted unlabeled samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5166c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_pseudo_labels(model, labeled_loader, validation_loader, device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    best_model_path = f\"../../models/pseudo_label/best_model_tabular_{config[\"training_session\"]}_pseudo.pt\"\n",
    "    best_accuracy = 0.00\n",
    "\n",
    "    # Fine-tune the model with pseudo-labeled data\n",
    "    for epoch in range(1, config[\"epochs\"] + 1):\n",
    "        print(f\"--- Start of Epoch {epoch}! ---\")\n",
    "\n",
    "        train_one_epoch(model, labeled_loader, device, optimizer)\n",
    "\n",
    "        predictions, labels = evaluate(model, validation_loader, device)\n",
    "        validation_accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "        if validation_accuracy > best_accuracy:\n",
    "            best_accuracy = validation_accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"âœ… Best model saved to {best_model_path} | Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "    print(f\"--- End of Training ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9070bb",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6fff04",
   "metadata": {},
   "source": [
    "This is the main script that runs the **Pseudo-Labeling training process** for tabular classification tasks. It follows a two-phase structure: initial supervised training, followed by fine-tuning with pseudo-labeled data.\n",
    "\n",
    "General Workflow\n",
    "1. **Device Setup**  \n",
    "   Automatically selects GPU if available, otherwise defaults to CPU.\n",
    "\n",
    "2. **Data Loading**  \n",
    "   Reads labeled and unlabeled tabular datasets from CSV files.\n",
    "\n",
    "3. **Train-Validation Split**  \n",
    "   Splits the labeled dataset into training and validation sets using stratified sampling.\n",
    "\n",
    "4. **Tokenization (Preprocessing)**  \n",
    "   - A `token_factory()` prepares the data:\n",
    "     - Encodes categorical columns.\n",
    "     - Normalizes numeric columns.\n",
    "     - Converts target labels into class indices.\n",
    "\n",
    "5. **Dataloader Creation**  \n",
    "   - `dataloader_factory()` builds three loaders:\n",
    "     - `labeled_loader`: For supervised training.\n",
    "     - `unlabeled_loader`: For pseudo-label generation.\n",
    "     - `validation_loader`: For tracking accuracy.\n",
    "\n",
    "6. **Model Initialization**  \n",
    "   Creates a tabular classification model (typically an MLP) based on input dimensions and number of classes.\n",
    "\n",
    "7. **Phase 1: Supervised Training**  \n",
    "   Trains the model using only labeled data via `train_without_pseudo_labels()`, then uses the trained model to generate **pseudo-labels** for confident unlabeled examples.\n",
    "\n",
    "8. **Pseudo-Label Filtering**  \n",
    "   If no confident predictions were found (based on `confidence_threshold`), the script exits early.\n",
    "\n",
    "9. **Dataset Combination**  \n",
    "   Combines original labeled data with pseudo-labeled examples using `torch.cat()` to create a new training set.\n",
    "   \n",
    "10. **Phase 2: Fine-Tuning with Pseudo-Labels**  \n",
    "    Re-trains the model using the expanded dataset via `train_with_pseudo_labels()`.\n",
    "<br><br>\n",
    "> ðŸ’¡ This pipeline is designed to boost performance when labeled data is limited but unlabeled data is abundant â€” a common scenario in real-world banking and business datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33112de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from token_factory import token_factory\n",
    "from model_factory import model_factory\n",
    "from dataloader_factory import dataloader_factory, combined_dataloader_factory\n",
    "\n",
    "# Load labeled and unlabeled dataset\n",
    "labeled_dataframe = pd.read_csv(config[\"labeled_dataset_path\"])\n",
    "unlabeled_dataframe = pd.read_csv(config[\"unlabeled_dataset_path\"])\n",
    "\n",
    "# Split labeled dataset into train and validation sets\n",
    "train_dataframe, validation_dataframe = train_test_split(\n",
    "    labeled_dataframe,\n",
    "    test_size=config[\"validation_set_percentage\"],\n",
    "    stratify=labeled_dataframe[config[\"tabular_target_column\"]],\n",
    "    random_state=config[\"seed\"]\n",
    ")\n",
    "\n",
    "# Obtain the tokenizer for tabular inputs\n",
    "tokenizer = token_factory(\n",
    "    categorical_columns=config[\"categorical_columns\"],\n",
    "    numeric_columns=config[\"numeric_columns\"],\n",
    "    target_column=config[\"tabular_target_column\"],\n",
    ")\n",
    "\n",
    "# Fit only on training dataframe\n",
    "tokenizer.fit(train_dataframe)\n",
    "\n",
    "# Tokenize features\n",
    "X_train = tokenizer.transform(train_dataframe)\n",
    "y_train = tokenizer.transform_target(train_dataframe)\n",
    "\n",
    "X_validation = tokenizer.transform(validation_dataframe)\n",
    "y_validation = tokenizer.transform_target(validation_dataframe)\n",
    "\n",
    "X_unlabeled = tokenizer.transform(unlabeled_dataframe)\n",
    "\n",
    "# Create dataloaders\n",
    "labeled_loader, unlabeled_loader, validation_loader = dataloader_factory(\n",
    "    X_train=X_train, y_train=y_train, \n",
    "    X_validation=X_validation, y_validation=y_validation, \n",
    "    X_unlabeled=X_unlabeled, batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "# Create MLP model\n",
    "input_dim = labeled_dataframe.drop(columns=[config[\"tabular_target_column\"]]).shape[1]\n",
    "num_classes = labeled_dataframe[config[\"tabular_target_column\"]].nunique()\n",
    "model = model_factory(\n",
    "    input_dim=input_dim,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "\n",
    "# Train on labeled data and generate pseudolabels\n",
    "X_pseudo, y_pseudo = train_without_pseudo_labels(model, labeled_loader, validation_loader, unlabeled_loader, device)\n",
    "\n",
    "# No pseudolabels were generated because model is not confident\n",
    "if len(X_pseudo) == 0:\n",
    "    print(\"No pseudo-labels generated\")\n",
    "    exit(0)\n",
    "\n",
    "# Combine generated pseudolabels with original labeled dataset\n",
    "X_combined = torch.cat([\n",
    "    torch.tensor(X_train, dtype=torch.float32),  \n",
    "    torch.cat(X_pseudo, dim=0)            \n",
    "], dim=0)\n",
    "y_combined = torch.cat([\n",
    "    torch.tensor(y_train, dtype=torch.long),\n",
    "    torch.cat(y_pseudo, dim=0)\n",
    "], dim=0)\n",
    "\n",
    "# Create dataloader for combined dataset\n",
    "labeled_loader = combined_dataloader_factory(\n",
    "    X_combined=X_combined, y_combined=y_combined, batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "# Train on labeled data with pseudolabels\n",
    "train_with_pseudo_labels(model, labeled_loader, validation_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
