{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad9b32ba",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c9e6a",
   "metadata": {},
   "source": [
    "Essential libraries for data handling, model training, and dataset preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ed212ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12877b1",
   "metadata": {},
   "source": [
    "Imports the custom factory modules used in the pipeline:\n",
    "- `token_factory`: Handles preprocessing/tokenization based on input type  \n",
    "- `dataloader_factory`: Builds dataloaders for labeled, unlabeled, and validation sets  \n",
    "- `model_factory`: Constructs models based on input type and task\n",
    "\n",
    "The modules are reloaded using `importlib.reload()` to reflect any recent changes without restarting the environment (useful in interactive environments like notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecde0b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import token_factory as tf\n",
    "import dataloader_factory as dl\n",
    "import model_factory as md\n",
    "\n",
    "importlib.reload(tf)\n",
    "importlib.reload(dl)\n",
    "importlib.reload(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc9653",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c34f31",
   "metadata": {},
   "source": [
    "This dictionary contains all the key settings for the training session. It is designed to support different input types â€” such as **images**, **text**, and **tabular data** â€” and allows you to toggle hyperparameters of the Mean Teacher algorithm.\n",
    "\n",
    "Sections:\n",
    "- **General**: Includes session ID and random seed for reproducibility.\n",
    "- **Mean Teacher Settings**: Controls core hyperparameters like learning rate, consistency loss weighting (`lambda_u`), EMA decay (`alpha`), and number of epochs.\n",
    "- **Dataset Paths & Structure**:\n",
    "  - Dataset paths should point to appropriate folders or files for labeled, unlabeled, and validation data.\n",
    "  - `input_type`: Specify \"image\", \"text\", or \"tabular\".\n",
    "  - `validation_set_percentage`: Used to split part of the labeled data for validation.\n",
    "- **Input-Specific Options**:\n",
    "  - For **image data**: Define the expected image resolution.\n",
    "  - For **text data**: Set the names of the column containing raw text and its corresponding label.\n",
    "  - For **tabular data**: Specify the column names for categorical and numeric inputs, as well as the target column. You can also define whether the target is a classification or regression problem.\n",
    "<br><br>\n",
    "> ðŸ’¡ Detailed explanations of configuration variables can be found in the README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ffd2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # General\n",
    "    \"training_session\": 1,\n",
    "    \"seed\": 27,\n",
    "\n",
    "    # Mean-Teacher Model\n",
    "    \"pre_trained\": False,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"alpha\": 0.99,\n",
    "    \"lambda_u\": 1.0,\n",
    "    \"epochs\": 20,\n",
    "\n",
    "    # Dataset\n",
    "    \"input_type\": \"\",                       \n",
    "    \"labeled_dataset_path\": \"\",\n",
    "    \"unlabeled_dataset_path\": \"\",\n",
    "    \"validation_set_percentage\": 0,\n",
    "    \"batch_size\": 64,\n",
    "\n",
    "    # Image input\n",
    "    \"image_size\": (0, 0),\n",
    "\n",
    "    # Text input\n",
    "    \"text_column\": \"\",\n",
    "    \"text_target_column\": \"\",\n",
    "\n",
    "    # Tabular input\n",
    "    \"categorical_columns\": [],\n",
    "    \"numeric_columns\": [],\n",
    "    \"tabular_target_column\": \"\",\n",
    "    \"is_tabular_target_categorical\": False, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9007a96",
   "metadata": {},
   "source": [
    "### Training Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6492ae73",
   "metadata": {},
   "source": [
    "***update_ema(...)***\n",
    "\n",
    "This function performs an **Exponential Moving Average (EMA)** update on the teacher model's weights, which is a core part of the **Mean Teacher** algorithm.\n",
    "\n",
    "How it works:\n",
    "- The teacher model is not trained directly.\n",
    "- Instead, it is updated slowly over time to be a smoothed version of the student model.\n",
    "- This is done by blending each parameter of the teacher with the corresponding parameter from the student, based on the value of `alpha`\n",
    "\n",
    "Parameters:\n",
    "- `student_model`: The main model being trained with labeled and unlabeled data.\n",
    "- `teacher_model`: A secondary model that receives EMA-updated weights from the student.\n",
    "- `alpha`: The smoothing factor (usually close to 1, e.g., 0.99), defined in the config. \n",
    "<br><br>\n",
    "> ðŸ’¡ This update makes the teacher more stable and helps the student learn consistent predictions over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "651b7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ema(student_model, teacher_model):\n",
    "    alpha = config[\"alpha\"]\n",
    "    for student_param, teacher_param in zip(student_model.parameters(), teacher_model.parameters()):\n",
    "        teacher_param.data = alpha * teacher_param.data + (1 - alpha) * student_param.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d230deae",
   "metadata": {},
   "source": [
    "***train_one_epoch(...)***\n",
    "\n",
    "This function performs **one training epoch** for both the student and teacher models using the **Mean Teacher** algorithm. It handles both **supervised learning** on labeled data and **consistency-based learning** on unlabeled data.\n",
    "\n",
    "How it works:\n",
    "\n",
    "1. **Load Data**:\n",
    "   - Fetch one batch each from the labeled and unlabeled dataloaders.\n",
    "   - For unlabeled data, two versions are used:\n",
    "     - `weak`: slightly augmented input for the teacher.\n",
    "     - `strong`: more heavily augmented input for the student.\n",
    "\n",
    "2. **Device Handling**:\n",
    "   - Inputs are moved to the GPU or CPU.\n",
    "   - Handles special cases for tokenized text (like BERT) where inputs are dictionaries.\n",
    "\n",
    "3. **Compute Supervised Loss**:\n",
    "   - Use labeled data to compute a loss between predictions and ground-truth labels.\n",
    "   - Chooses between MSE (for regression) or CrossEntropy (for classification).\n",
    "\n",
    "4. **Compute Unsupervised Loss (Consistency Loss)**:\n",
    "   - The teacher generates predictions (pseudo-labels) from the weakly augmented input.\n",
    "   - The student is trained to match these pseudo-labels using the strongly augmented version.\n",
    "   - Uses MSE between softmax probabilities if it's a classification task.\n",
    "\n",
    "5. **Combine Losses**:\n",
    "   - The total loss is a weighted sum of supervised and unsupervised losses.\n",
    "   - `lambda_u` controls the influence of the unsupervised component.\n",
    "\n",
    "6. **Backpropagation and Optimization**:\n",
    "   - Gradients are computed from the total loss and used to update the student model.\n",
    "\n",
    "7. **Teacher Update via EMA**:\n",
    "   - The teacher model is updated to slowly follow the student model using Exponential Moving Average.\n",
    "\n",
    "Parameters:\n",
    "- `student_model`: The main model that is actively trained.\n",
    "- `teacher_model`: A stable copy of the student model, updated with EMA.\n",
    "- `labeled_loader`: Dataloader for labeled samples.\n",
    "- `unlabeled_loader`: Dataloader that returns both weak and strong augmented unlabeled inputs.\n",
    "- `optimizer`: Optimizer for student model.\n",
    "- `device`: Target device (CPU/GPU).\n",
    "- `epoch`: Current training epoch (for logging/debugging).\n",
    "- `is_regression`: Boolean flag to switch loss functions for regression tasks.\n",
    "\n",
    "<br>\n",
    "\n",
    "> ðŸ’¡ This function is designed to support **both regression and classification**, and works with all three input types: images, text, and tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "75d350fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(student_model, teacher_model, labeled_loader, unlabeled_loader, optimizer, device, epoch, is_regression):\n",
    "    student_model.train()\n",
    "    teacher_model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for (x_labeled, y_labeled), (x_unlabeled_weak, x_unlabeled_strong) in zip(labeled_loader, unlabeled_loader):\n",
    "        if config[\"input_type\"] == \"text\" and config[\"pre_trained\"]:\n",
    "            x_labeled = {k: v.to(device) for k, v in x_labeled.items()}\n",
    "            x_unlabeled_weak = {k: v.to(device) for k, v in x_unlabeled_weak.items()}\n",
    "            x_unlabeled_strong = {k: v.to(device) for k, v in x_unlabeled_strong.items()}\n",
    "        else:\n",
    "            x_labeled = x_labeled.to(device)\n",
    "            x_unlabeled_weak = x_unlabeled_weak.to(device)\n",
    "            x_unlabeled_strong = x_unlabeled_strong.to(device)\n",
    "\n",
    "        y_labeled = y_labeled.to(device)\n",
    "        if is_regression:\n",
    "            y_labeled = y_labeled.float().unsqueeze(1).to(device)\n",
    "        else:\n",
    "            y_labeled = y_labeled.to(device)\n",
    "\n",
    "        # Supervised loss\n",
    "        logits_labeled = student_model(x_labeled)\n",
    "        supervised_loss = F.mse_loss(logits_labeled, y_labeled) if is_regression else F.cross_entropy(logits_labeled, y_labeled)\n",
    "\n",
    "        # Unsupervised loss (consistency)\n",
    "        if is_regression:\n",
    "            with torch.no_grad():\n",
    "                pseudo_labels = teacher_model(x_unlabeled_weak)\n",
    "            logits_unlabeled_strong = student_model(x_unlabeled_strong)\n",
    "            unsupervised_loss = F.mse_loss(logits_unlabeled_strong, pseudo_labels)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                logits_ulb_w = teacher_model(x_unlabeled_weak)\n",
    "                pseudo_labels = torch.softmax(logits_ulb_w, dim=1)\n",
    "            logits_unlabeled_strong = student_model(x_unlabeled_strong)\n",
    "            unsupervised_loss = F.mse_loss(torch.softmax(logits_unlabeled_strong, dim=1), pseudo_labels)\n",
    "\n",
    "        # Total loss\n",
    "        loss = supervised_loss + config[\"lambda_u\"] * unsupervised_loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # EMA update\n",
    "        update_ema(student_model, teacher_model)\n",
    "\n",
    "    print(f\"Total Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0719e2b",
   "metadata": {},
   "source": [
    "***evaluate(...)***\n",
    "\n",
    "This function evaluates the performance of a trained model on a **validation dataset**. It works for both **regression** and **classification** tasks.\n",
    "\n",
    "How it works:\n",
    "\n",
    "1. **Switches to Evaluation Mode**:\n",
    "   - Disables dropout and gradient tracking for faster and consistent evaluation.\n",
    "\n",
    "2. **Iterates Through Validation Batches**:\n",
    "   - Inputs are passed through the model.\n",
    "   - Loss is computed using:\n",
    "     - **MSE** (Mean Squared Error) for regression\n",
    "     - **Cross-Entropy** for classification\n",
    "\n",
    "3. **Generates Predictions**:\n",
    "   - For regression: uses raw predicted values.\n",
    "   - For classification: uses the class with the highest predicted score.\n",
    "\n",
    "4. **Computes Metrics**:\n",
    "   - For regression: Mean Absolute Error (MAE) and total loss\n",
    "   - For classification: Accuracy and total loss\n",
    "\n",
    "Parameters:\n",
    "- `model`: The model to evaluate (student or teacher).\n",
    "- `validation_loader`: Dataloader for the validation dataset.\n",
    "- `device`: The target device (CPU or GPU).\n",
    "- `is_regression`: Boolean flag that determines which evaluation metrics and loss function to use.\n",
    "<br><br>\n",
    "> ðŸ’¡ Automatically handles different input types (text, tabular, image), and supports pretrained tokenizers for text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e89430b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, validation_loader, device, is_regression):\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions, all_labels = [], []\n",
    "    total_loss = 0.00\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in validation_loader:\n",
    "            if config[\"input_type\"] == \"text\" and config[\"pre_trained\"]:\n",
    "                x = {k: v.to(device) for k, v in x.items()}\n",
    "            else:\n",
    "                x = x.to(device)\n",
    "\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "            if is_regression:\n",
    "                loss = F.mse_loss(logits.squeeze(), y.float())\n",
    "                predictions = logits.squeeze()\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits, y.long())\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    if is_regression:\n",
    "        mae = np.mean(np.abs(np.array(all_predictions) - np.array(all_labels)))\n",
    "        print(f\"Validation MAE: {mae:.4f} | Loss: {total_loss:.4f}\")\n",
    "        return mae, total_loss\n",
    "    else:\n",
    "        accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f} | Loss: {total_loss:.4f}\")\n",
    "        return accuracy, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e99a9",
   "metadata": {},
   "source": [
    "***train_mean_teacher(...)***\n",
    "\n",
    "This is the **main training function** for the **Mean Teacher** algorithm. It trains a student model using both labeled and unlabeled data, while maintaining a teacher model that is updated via Exponential Moving Average (EMA).\n",
    "\n",
    "How it works:\n",
    "\n",
    "1. **Initialize Teacher Model**:\n",
    "   - A copy of the student model is made using `deepcopy`.\n",
    "   - The teacherâ€™s weights are frozen (`requires_grad = False`) so it's only updated via EMA.\n",
    "\n",
    "2. **Set Up Training**:\n",
    "   - Uses the Adam optimizer with a configurable learning rate.\n",
    "   - Checks if the task is **regression** or **classification**, based on the config flags for tabular targets.\n",
    "\n",
    "3. **Training Loop**:\n",
    "   For each epoch:\n",
    "   - Calls `train_one_epoch()` to train the student model and update the teacher.\n",
    "   - Evaluates the student model on the validation set.\n",
    "\n",
    "4. **Model Checkpointing**:\n",
    "   - For **regression**, the best model is saved based on the **lowest MAE (Mean Absolute Error)**.\n",
    "   - For **classification**, the best model is saved based on the **highest accuracy**.\n",
    "   - Saves the model to a session-specific file path for reproducibility.\n",
    "\n",
    "5. **Logging**:\n",
    "   - Prints epoch-level progress and saves the model only when improvement is detected.\n",
    "\n",
    "Parameters:\n",
    "- `student_model`: The model actively trained.\n",
    "- `labeled_loader`: Dataloader with labeled samples.\n",
    "- `unlabeled_loader`: Dataloader with weak/strong augmented unlabeled samples.\n",
    "- `validation_loader`: Dataloader for model evaluation.\n",
    "- `device`: Target device (CPU or GPU).\n",
    "<br><br>\n",
    "> ðŸ’¡ This function integrates all core parts of the Mean Teacher loop: supervised learning, unsupervised consistency loss, teacher EMA updates, and validation tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e4464467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mean_teacher(student_model, labeled_loader, unlabeled_loader, validation_loader, device):\n",
    "    teacher_model = copy.deepcopy(student_model)\n",
    "    for param in teacher_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    optimizer = optim.Adam(student_model.parameters(), lr=config[\"learning_rate\"])\n",
    "    is_regression = True if (config[\"input_type\"] == \"tabular\" and not config[\"is_tabular_target_categorical\"]) else False\n",
    "\n",
    "    best_val_accuracy, best_mae = 0, float(\"inf\")\n",
    "    best_model_path = f\"../../models/mean_teacher/best_model_{config[\"input_type\"]}_{config[\"training_session\"]}.pt\"\n",
    "    for epoch in range(1, config[\"epochs\"] + 1):\n",
    "        print(f\"--- Start of Epoch {epoch} ---\")\n",
    "        \n",
    "        train_one_epoch(\n",
    "            student_model, teacher_model, labeled_loader, unlabeled_loader, optimizer, device, epoch, is_regression\n",
    "        )\n",
    "\n",
    "        if is_regression:\n",
    "            mae, _ = evaluate(student_model, validation_loader, device, is_regression)\n",
    "            if mae < best_mae:\n",
    "                best_mae = mae\n",
    "                torch.save(student_model.state_dict(), best_model_path)\n",
    "                print(f\"âœ… Best model saved to {best_model_path} | MAE: {mae:.4f}\")\n",
    "        else:\n",
    "            validation_accuracy, _ = evaluate(student_model, validation_loader, device, is_regression)\n",
    "            if validation_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = validation_accuracy\n",
    "                torch.save(student_model.state_dict(), best_model_path)\n",
    "                print(f\"âœ… Best model saved to {best_model_path} | Accuracy: {validation_accuracy:.4f}\")\n",
    "\n",
    "    print(\"--- End of Training ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066d4fe",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fdfc81",
   "metadata": {},
   "source": [
    "This is the **main execution block** that sets up the correct data pipeline and model based on the `input_type` defined in the configuration. It supports three types of input data:\n",
    "\n",
    "- `\"image\"` â€“ for document or visual classification tasks  \n",
    "- `\"text\"` â€“ for NLP tasks like intent classification or document tagging  \n",
    "- `\"tabular\"` â€“ for structured data like customer profiles or financial metrics\n",
    "\n",
    "General Workflow\n",
    "\n",
    "1. **Detect Device**  \n",
    "   Automatically selects GPU if available, otherwise falls back to CPU.\n",
    "\n",
    "2. **Load Input Data**  \n",
    "   Reads from CSV files or image folders depending on input type.\n",
    "\n",
    "3. **Train-Validation Split**  \n",
    "   Partitions labeled data into a training and validation set using the percentage defined in `config[\"validation_set_percentage\"]`.\n",
    "\n",
    "4. **Tokenizer Factory**  \n",
    "   Uses a unified `token_factory()` to preprocess:\n",
    "   - Images (resize, normalize)\n",
    "   - Text (tokenize using TF-IDF or a pre-trained model like BERT)\n",
    "   - Tabular (handle categorical/numerical features, scaling)\n",
    "\n",
    "5. **Dataloader Creation**  \n",
    "   Uses `dataloader_factory()` to return:\n",
    "   - `labeled_loader`: for supervised learning\n",
    "   - `unlabeled_loader`: for consistency learning\n",
    "   - `validation_loader`: for performance tracking\n",
    "\n",
    "6. **Model Factory**  \n",
    "   Automatically builds the right model:\n",
    "   - CNN or ResNet for images\n",
    "   - MLP or BERT for text\n",
    "   - MLP for tabular data\n",
    "\n",
    "7. **Run Mean Teacher Training**  \n",
    "   Starts the full semi-supervised training cycle using the `train_mean_teacher()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "33112de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from token_factory import token_factory\n",
    "from dataloader_factory import dataloader_factory\n",
    "from model_factory import model_factory\n",
    "\n",
    "if config[\"input_type\"] == \"image\":\n",
    "    # Load labeled and unlabeled dataset \n",
    "    labeled_dataset = ImageFolder(root=config[\"labeled_dataset_path\"])\n",
    "    unlabeled_dataset = ImageFolder(root=config[\"unlabeled_dataset_path\"])\n",
    "\n",
    "    # Split labeled dataset into train and validation sets\n",
    "    indices = list(range(len(labeled_dataset)))\n",
    "    labels = [sample[1] for sample in labeled_dataset.samples]\n",
    "    train_indices, validation_indices = train_test_split(\n",
    "        indices,\n",
    "        test_size=config[\"validation_set_percentage\"],\n",
    "        stratify=labels,\n",
    "        random_state=config[\"seed\"]\n",
    "    )\n",
    "\n",
    "    # Obtain the base transform for image inputs\n",
    "    tokenizer = token_factory(\n",
    "        \"image\", \n",
    "        image_size=config[\"image_size\"]\n",
    "    )\n",
    "\n",
    "    train = tokenizer(Subset(labeled_dataset, train_indices))\n",
    "    validation = tokenizer(Subset(labeled_dataset, validation_indices))\n",
    "    unlabeled = tokenizer(unlabeled_dataset)\n",
    "\n",
    "    # Create dataloaders\n",
    "    labeled_loader, unlabeled_loader, validation_loader = dataloader_factory(\n",
    "        \"image\",\n",
    "        train=train, validation=validation, \n",
    "        unlabeled=unlabeled, batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    # Create ResNet or CNN model\n",
    "    model = model_factory(\n",
    "        \"image\", \n",
    "        num_classes=len(labeled_dataset.classes),\n",
    "        pretrained=config[\"pre_trained\"]\n",
    "    ).to(device)\n",
    "\n",
    "    train_mean_teacher(\n",
    "        model, labeled_loader, unlabeled_loader, validation_loader, device\n",
    "    )\n",
    "\n",
    "elif config[\"input_type\"] == \"text\":\n",
    "    # Load labeled and unlabeled dataset\n",
    "    labeled_dataframe = pd.read_csv(config[\"labeled_dataset_path\"])\n",
    "    unlabeled_dataframe = pd.read_csv(config[\"unlabeled_dataset_path\"])\n",
    "\n",
    "    # Split labeled dataset into train and validation sets\n",
    "    train_dataframe, validation_dataframe = train_test_split(\n",
    "        labeled_dataframe,\n",
    "        test_size=config[\"validation_set_percentage\"],\n",
    "        stratify=labeled_dataframe[config[\"text_target_column\"]],\n",
    "        random_state=config[\"seed\"]\n",
    "    )\n",
    "\n",
    "    # Obtain the tokenizer for text inputs\n",
    "    tokenizer = token_factory(\n",
    "        \"text\",\n",
    "        text_column=config[\"text_column\"],\n",
    "        target_column=config[\"text_target_column\"],\n",
    "        pretrained=config[\"pre_trained\"],\n",
    "    )\n",
    "\n",
    "    # Fit only on training dataframe (if not pre-trained)\n",
    "    tokenizer.fit(train_dataframe)  \n",
    "\n",
    "    # Transform remaining dataframes\n",
    "    X_train = tokenizer.transform(train_dataframe)\n",
    "    y_train = tokenizer.transform_target(train_dataframe)\n",
    "\n",
    "    X_validation = tokenizer.transform(validation_dataframe)\n",
    "    y_validation = tokenizer.transform_target(validation_dataframe)\n",
    "\n",
    "    # Unlabeled text will be transformed later in dataloader_factory\n",
    "    X_unlabeled = unlabeled_dataframe[config[\"text_column\"]].tolist()\n",
    "\n",
    "    # Create dataloaders\n",
    "    labeled_loader, unlabeled_loader, validation_loader = dataloader_factory(\n",
    "        \"text\",\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        X_validation=X_validation, y_validation=y_validation,\n",
    "        X_unlabeled=X_unlabeled, tokenizer=tokenizer, \n",
    "        batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    # Create BERT model\n",
    "    num_classes = len(np.unique(y_train.numpy())) \n",
    "    input_dim = X_train.shape[1] if not config[\"pre_trained\"] else None \n",
    "    model = model_factory(\n",
    "        \"text\",\n",
    "        num_classes=num_classes,\n",
    "        pretrained=config[\"pre_trained\"],\n",
    "        tfidf_dim=input_dim\n",
    "    ).to(device)\n",
    "\n",
    "    train_mean_teacher(\n",
    "        model, labeled_loader, unlabeled_loader, validation_loader, device\n",
    "    )\n",
    "\n",
    "elif config[\"input_type\"] == \"tabular\":\n",
    "    is_regression = not config[\"is_tabular_target_categorical\"]\n",
    "\n",
    "    # Load labeled and unlabeled dataset\n",
    "    labeled_dataframe = pd.read_csv(config[\"labeled_dataset_path\"])\n",
    "    unlabeled_dataframe = pd.read_csv(config[\"unlabeled_dataset_path\"])\n",
    "\n",
    "    # Split labeled dataset into train and validation sets\n",
    "    train_dataframe, validation_dataframe = train_test_split(\n",
    "        labeled_dataframe,\n",
    "        test_size=config[\"validation_set_percentage\"],\n",
    "        stratify=labeled_dataframe[config[\"tabular_target_column\"]],\n",
    "        random_state=config[\"seed\"]\n",
    "    )\n",
    "\n",
    "    # Obtain the tokenizer for tabular inputs\n",
    "    tokenizer = token_factory(\n",
    "        \"tabular\", \n",
    "        categorical_columns=config[\"categorical_columns\"],\n",
    "        numeric_columns=config[\"numeric_columns\"],\n",
    "        target_column=config[\"tabular_target_column\"],\n",
    "        is_target_categorical=config[\"is_tabular_target_categorical\"]\n",
    "    )\n",
    "\n",
    "    # Fit only on training dataframe\n",
    "    tokenizer.fit(train_dataframe)\n",
    "\n",
    "    # Transform remaining dataframes\n",
    "    X_train = tokenizer.transform(train_dataframe)\n",
    "    y_train = tokenizer.transform_target(train_dataframe)\n",
    "\n",
    "    X_validation = tokenizer.transform(validation_dataframe)\n",
    "    y_validation = tokenizer.transform_target(validation_dataframe)\n",
    "\n",
    "    X_unlabeled = tokenizer.transform(unlabeled_dataframe)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_validation = torch.tensor(X_validation, dtype=torch.float32)\n",
    "\n",
    "    if is_regression:\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "        y_validation = torch.tensor(y_validation.to_numpy(), dtype=torch.float32 if not config[\"is_tabular_target_categorical\"] else torch.long)\n",
    "    else:\n",
    "        y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_validation = torch.tensor(y_validation, dtype=torch.float32 if not config[\"is_tabular_target_categorical\"] else torch.long)\n",
    "\n",
    "    X_unlabeled = torch.tensor(X_unlabeled, dtype=torch.float32)\n",
    "\n",
    "    # Create dataloaders\n",
    "    labeled_loader, unlabeled_loader, validation_loader = dataloader_factory(\n",
    "        \"tabular\", \n",
    "        X_train=X_train, y_train=y_train, \n",
    "        X_validation=X_validation, y_validation=y_validation, \n",
    "        X_unlabeled=X_unlabeled, batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "    \n",
    "    # Create MLP model\n",
    "    input_dim = labeled_dataframe.drop(columns=[config[\"tabular_target_column\"]]).shape[1]\n",
    "    num_classes = labeled_dataframe[config[\"tabular_target_column\"]].nunique()\n",
    "    model = model_factory(\n",
    "        \"tabular\",\n",
    "        input_dim=input_dim,\n",
    "        num_classes=num_classes,\n",
    "        regression=is_regression\n",
    "    ).to(device)\n",
    "\n",
    "    train_mean_teacher(\n",
    "        model, labeled_loader, unlabeled_loader, validation_loader, device\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported input type: {config[\"input_type\"]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
