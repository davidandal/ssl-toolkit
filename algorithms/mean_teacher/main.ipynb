{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad9b32ba",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ed212ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ecde0b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model_factory' from '/Users/dundale/Downloads/bpi-ssl/algorithms/mean_teacher/model_factory.py'>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import token_factory as tf\n",
    "import dataloader_factory as dl\n",
    "import model_factory as md\n",
    "\n",
    "importlib.reload(tf)\n",
    "importlib.reload(dl)\n",
    "importlib.reload(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc9653",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ffd2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # General\n",
    "    \"training_session\": 1,\n",
    "    \"seed\": 27,\n",
    "\n",
    "    # Mean-Teacher Model\n",
    "    \"pre_trained\": False,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"alpha\": 0.99,\n",
    "    \"lambda_u\": 1.0,\n",
    "    \"epochs\": 20,\n",
    "\n",
    "    # Dataset\n",
    "    \"input_type\": \"\",                       \n",
    "    \"labeled_dataset_path\": \"\",\n",
    "    \"unlabeled_dataset_path\": \"\",\n",
    "    \"validation_set_percentage\": 0,\n",
    "    \"batch_size\": 64,\n",
    "\n",
    "    # Image input\n",
    "    \"image_size\": (0, 0),\n",
    "\n",
    "    # Text input\n",
    "    \"text_column\": \"\",\n",
    "    \"text_target_column\": \"\",\n",
    "\n",
    "    # Tabular input\n",
    "    \"categorical_columns\": [],\n",
    "    \"numeric_columns\": [],\n",
    "    \"tabular_target_column\": \"\",\n",
    "    \"is_tabular_target_categorical\": False, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9007a96",
   "metadata": {},
   "source": [
    "### Training Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "651b7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ema(student_model, teacher_model):\n",
    "    alpha = config[\"alpha\"]\n",
    "    for student_param, teacher_param in zip(student_model.parameters(), teacher_model.parameters()):\n",
    "        teacher_param.data = alpha * teacher_param.data + (1 - alpha) * student_param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "75d350fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(student_model, teacher_model, labeled_loader, unlabeled_loader, optimizer, device, epoch, is_regression):\n",
    "    student_model.train()\n",
    "    teacher_model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for (x_labeled, y_labeled), (x_unlabeled_weak, x_unlabeled_strong) in zip(labeled_loader, unlabeled_loader):\n",
    "        if config[\"input_type\"] == \"text\" and config[\"pre_trained\"]:\n",
    "            x_labeled = {k: v.to(device) for k, v in x_labeled.items()}\n",
    "            x_unlabeled_weak = {k: v.to(device) for k, v in x_unlabeled_weak.items()}\n",
    "            x_unlabeled_strong = {k: v.to(device) for k, v in x_unlabeled_strong.items()}\n",
    "        else:\n",
    "            x_labeled = x_labeled.to(device)\n",
    "            x_unlabeled_weak = x_unlabeled_weak.to(device)\n",
    "            x_unlabeled_strong = x_unlabeled_strong.to(device)\n",
    "\n",
    "        y_labeled = y_labeled.to(device)\n",
    "        if is_regression:\n",
    "            y_labeled = y_labeled.float().unsqueeze(1).to(device)\n",
    "        else:\n",
    "            y_labeled = y_labeled.to(device)\n",
    "\n",
    "        # Supervised loss\n",
    "        logits_labeled = student_model(x_labeled)\n",
    "        supervised_loss = F.mse_loss(logits_labeled, y_labeled) if is_regression else F.cross_entropy(logits_labeled, y_labeled)\n",
    "\n",
    "        # Unsupervised loss (consistency)\n",
    "        if is_regression:\n",
    "            with torch.no_grad():\n",
    "                pseudo_labels = teacher_model(x_unlabeled_weak)\n",
    "            logits_unlabeled_strong = student_model(x_unlabeled_strong)\n",
    "            unsupervised_loss = F.mse_loss(logits_unlabeled_strong, pseudo_labels)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                logits_ulb_w = teacher_model(x_unlabeled_weak)\n",
    "                pseudo_labels = torch.softmax(logits_ulb_w, dim=1)\n",
    "            logits_unlabeled_strong = student_model(x_unlabeled_strong)\n",
    "            unsupervised_loss = F.mse_loss(torch.softmax(logits_unlabeled_strong, dim=1), pseudo_labels)\n",
    "\n",
    "        # Total loss\n",
    "        loss = supervised_loss + config[\"lambda_u\"] * unsupervised_loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # EMA update\n",
    "        update_ema(student_model, teacher_model)\n",
    "\n",
    "    print(f\"Total Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e89430b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, validation_loader, device, is_regression):\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions, all_labels = [], []\n",
    "    total_loss = 0.00\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in validation_loader:\n",
    "            if config[\"input_type\"] == \"text\" and config[\"pre_trained\"]:\n",
    "                x = {k: v.to(device) for k, v in x.items()}\n",
    "            else:\n",
    "                x = x.to(device)\n",
    "\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "            if is_regression:\n",
    "                loss = F.mse_loss(logits.squeeze(), y.float())\n",
    "                predictions = logits.squeeze()\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits, y.long())\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    if is_regression:\n",
    "        mae = np.mean(np.abs(np.array(all_predictions) - np.array(all_labels)))\n",
    "        print(f\"Validation MAE: {mae:.4f} | Loss: {total_loss:.4f}\")\n",
    "        return mae, total_loss\n",
    "    else:\n",
    "        accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f} | Loss: {total_loss:.4f}\")\n",
    "        return accuracy, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e4464467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mean_teacher(student_model, labeled_loader, unlabeled_loader, validation_loader, device):\n",
    "    teacher_model = copy.deepcopy(student_model)\n",
    "    for param in teacher_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    optimizer = optim.Adam(student_model.parameters(), lr=config[\"learning_rate\"])\n",
    "    is_regression = True if (config[\"input_type\"] == \"tabular\" and not config[\"is_tabular_target_categorical\"]) else False\n",
    "\n",
    "    best_val_accuracy, best_mae = 0, float(\"inf\")\n",
    "    best_model_path = f\"../../models/mean_teacher/best_model_{config[\"input_type\"]}_{config[\"training_session\"]}.pt\"\n",
    "    for epoch in range(1, config[\"epochs\"] + 1):\n",
    "        print(f\"--- Start of Epoch {epoch} ---\")\n",
    "        \n",
    "        train_one_epoch(\n",
    "            student_model, teacher_model, labeled_loader, unlabeled_loader, optimizer, device, epoch, is_regression\n",
    "        )\n",
    "\n",
    "        if is_regression:\n",
    "            mae, _ = evaluate(student_model, validation_loader, device, is_regression)\n",
    "            if mae < best_mae:\n",
    "                best_mae = mae\n",
    "                torch.save(student_model.state_dict(), best_model_path)\n",
    "                print(f\"✅ Best model saved to {best_model_path} | MAE: {mae:.4f}\")\n",
    "        else:\n",
    "            validation_accuracy, _ = evaluate(student_model, validation_loader, device, is_regression)\n",
    "            if validation_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = validation_accuracy\n",
    "                torch.save(student_model.state_dict(), best_model_path)\n",
    "                print(f\"✅ Best model saved to {best_model_path} | Accuracy: {validation_accuracy:.4f}\")\n",
    "\n",
    "    print(\"--- End of Training ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066d4fe",
   "metadata": {},
   "source": [
    "### Training Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33112de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from token_factory import token_factory\n",
    "from dataloader_factory import dataloader_factory\n",
    "from model_factory import model_factory\n",
    "\n",
    "if config[\"input_type\"] == \"image\":\n",
    "    # Load labeled and unlabeled dataset \n",
    "    labeled_dataset = ImageFolder(root=config[\"labeled_dataset_path\"])\n",
    "    unlabeled_dataset = ImageFolder(root=config[\"unlabeled_dataset_path\"])\n",
    "\n",
    "    # Split labeled dataset into train and validation sets\n",
    "    indices = list(range(len(labeled_dataset)))\n",
    "    labels = [sample[1] for sample in labeled_dataset.samples]\n",
    "    train_indices, validation_indices = train_test_split(\n",
    "        indices,\n",
    "        test_size=config[\"validation_set_percentage\"],\n",
    "        stratify=labels,\n",
    "        random_state=config[\"seed\"]\n",
    "    )\n",
    "\n",
    "    # Obtain the base transform for image inputs\n",
    "    tokenizer = token_factory(\n",
    "        \"image\", \n",
    "        image_size=config[\"image_size\"]\n",
    "    )\n",
    "\n",
    "    train = tokenizer(Subset(labeled_dataset, train_indices))\n",
    "    validation = tokenizer(Subset(labeled_dataset, validation_indices))\n",
    "    unlabeled = tokenizer(unlabeled_dataset)\n",
    "\n",
    "    # Create dataloaders\n",
    "    labeled_loader, unlabeled_loader, validation_loader = dataloader_factory(\n",
    "        \"image\",\n",
    "        train=train, validation=validation, \n",
    "        unlabeled=unlabeled, batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    # Create ResNet or CNN model\n",
    "    model = model_factory(\n",
    "        \"image\", \n",
    "        num_classes=len(labeled_dataset.classes),\n",
    "        pretrained=config[\"pre_trained\"]\n",
    "    ).to(device)\n",
    "\n",
    "    train_mean_teacher(\n",
    "        model, labeled_loader, unlabeled_loader, validation_loader, device\n",
    "    )\n",
    "\n",
    "elif config[\"input_type\"] == \"text\":\n",
    "    # Load labeled and unlabeled dataset\n",
    "    labeled_dataframe = pd.read_csv(config[\"labeled_dataset_path\"])\n",
    "    unlabeled_dataframe = pd.read_csv(config[\"unlabeled_dataset_path\"])\n",
    "\n",
    "    # Split labeled dataset into train and validation sets\n",
    "    train_dataframe, validation_dataframe = train_test_split(\n",
    "        labeled_dataframe,\n",
    "        test_size=config[\"validation_set_percentage\"],\n",
    "        stratify=labeled_dataframe[config[\"text_target_column\"]],\n",
    "        random_state=config[\"seed\"]\n",
    "    )\n",
    "\n",
    "    # Obtain the tokenizer for text inputs\n",
    "    tokenizer = token_factory(\n",
    "        \"text\",\n",
    "        text_column=config[\"text_column\"],\n",
    "        target_column=config[\"text_target_column\"],\n",
    "        pretrained=config[\"pre_trained\"],\n",
    "    )\n",
    "\n",
    "    # Fit only on training dataframe (if not pre-trained)\n",
    "    tokenizer.fit(train_dataframe)  \n",
    "\n",
    "    # Transform remaining dataframes\n",
    "    X_train = tokenizer.transform(train_dataframe)\n",
    "    y_train = tokenizer.transform_target(train_dataframe)\n",
    "\n",
    "    X_validation = tokenizer.transform(validation_dataframe)\n",
    "    y_validation = tokenizer.transform_target(validation_dataframe)\n",
    "\n",
    "    # Unlabeled text will be transformed later in dataloader_factory\n",
    "    X_unlabeled = unlabeled_dataframe[config[\"text_column\"]].tolist()\n",
    "\n",
    "    # Create dataloaders\n",
    "    labeled_loader, unlabeled_loader, validation_loader = dataloader_factory(\n",
    "        \"text\",\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        X_validation=X_validation, y_validation=y_validation,\n",
    "        X_unlabeled=X_unlabeled, tokenizer=tokenizer, \n",
    "        batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    # Create BERT model\n",
    "    num_classes = len(np.unique(y_train.numpy())) \n",
    "    input_dim = X_train.shape[1] if not config[\"pre_trained\"] else None \n",
    "    model = model_factory(\n",
    "        \"text\",\n",
    "        num_classes=num_classes,\n",
    "        pretrained=config[\"pre_trained\"],\n",
    "        tfidf_dim=input_dim\n",
    "    ).to(device)\n",
    "\n",
    "    train_mean_teacher(\n",
    "        model, labeled_loader, unlabeled_loader, validation_loader, device\n",
    "    )\n",
    "\n",
    "elif config[\"input_type\"] == \"tabular\":\n",
    "    is_regression = not config[\"is_tabular_target_categorical\"]\n",
    "\n",
    "    # Load labeled and unlabeled dataset\n",
    "    labeled_dataframe = pd.read_csv(config[\"labeled_dataset_path\"])\n",
    "    unlabeled_dataframe = pd.read_csv(config[\"unlabeled_dataset_path\"])\n",
    "\n",
    "    # Split labeled dataset into train and validation sets\n",
    "    train_dataframe, validation_dataframe = train_test_split(\n",
    "        labeled_dataframe,\n",
    "        test_size=config[\"validation_set_percentage\"],\n",
    "        stratify=labeled_dataframe[config[\"tabular_target_column\"]],\n",
    "        random_state=config[\"seed\"]\n",
    "    )\n",
    "\n",
    "    # Obtain the tokenizer for tabular inputs\n",
    "    tokenizer = token_factory(\n",
    "        \"tabular\", \n",
    "        categorical_columns=config[\"categorical_columns\"],\n",
    "        numeric_columns=config[\"numeric_columns\"],\n",
    "        target_column=config[\"tabular_target_column\"],\n",
    "        is_target_categorical=config[\"is_tabular_target_categorical\"]\n",
    "    )\n",
    "\n",
    "    # Fit only on training dataframe\n",
    "    tokenizer.fit(train_dataframe)\n",
    "\n",
    "    # Transform remaining dataframes\n",
    "    X_train = tokenizer.transform(train_dataframe)\n",
    "    y_train = tokenizer.transform_target(train_dataframe)\n",
    "\n",
    "    X_validation = tokenizer.transform(validation_dataframe)\n",
    "    y_validation = tokenizer.transform_target(validation_dataframe)\n",
    "\n",
    "    X_unlabeled = tokenizer.transform(unlabeled_dataframe)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_validation = torch.tensor(X_validation, dtype=torch.float32)\n",
    "\n",
    "    if is_regression:\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "        y_validation = torch.tensor(y_validation.to_numpy(), dtype=torch.float32 if not config[\"is_tabular_target_categorical\"] else torch.long)\n",
    "    else:\n",
    "        y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_validation = torch.tensor(y_validation, dtype=torch.float32 if not config[\"is_tabular_target_categorical\"] else torch.long)\n",
    "\n",
    "    X_unlabeled = torch.tensor(X_unlabeled, dtype=torch.float32)\n",
    "\n",
    "    # Create dataloaders\n",
    "    labeled_loader, unlabeled_loader, validation_loader = dataloader_factory(\n",
    "        \"tabular\", \n",
    "        X_train=X_train, y_train=y_train, \n",
    "        X_validation=X_validation, y_validation=y_validation, \n",
    "        X_unlabeled=X_unlabeled, batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "    \n",
    "    # Create MLP model\n",
    "    input_dim = labeled_dataframe.drop(columns=[config[\"tabular_target_column\"]]).shape[1]\n",
    "    num_classes = labeled_dataframe[config[\"tabular_target_column\"]].nunique()\n",
    "    model = model_factory(\n",
    "        \"tabular\",\n",
    "        input_dim=input_dim,\n",
    "        num_classes=num_classes,\n",
    "        regression=is_regression\n",
    "    ).to(device)\n",
    "\n",
    "    train_mean_teacher(\n",
    "        model, labeled_loader, unlabeled_loader, validation_loader, device\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported input type: {config[\"input_type\"]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
