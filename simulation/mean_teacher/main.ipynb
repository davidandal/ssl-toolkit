{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ed212ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "f0ffd2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     # General\n",
    "#     \"training_session\": 2,\n",
    "\n",
    "#     # Mean-Teacher Model\n",
    "#     \"pre_trained\": True,\n",
    "#     \"learning_rate\": 3e-4,\n",
    "#     \"alpha\": 0.99,\n",
    "#     \"lambda_u\": 1.0,\n",
    "#     \"epochs\": 10,\n",
    "\n",
    "#     # Dataset\n",
    "#     \"input_type\": \"tabular\",\n",
    "#     \"dataset_path\": \"../../datasets/credit_data.csv\",\n",
    "#     \"num_labels\": 0.6,\n",
    "#     \"batch_size\": 64,\n",
    "\n",
    "#     # Image input\n",
    "#     \"image_classes\": [\"form\", \"invoice\", \"memo\", \"letter\"],\n",
    "#     \"image_size\": 224,\n",
    "\n",
    "#     # Text input\n",
    "\n",
    "#     # Tabular input\n",
    "#     \"categorical_columns\": [\n",
    "#         \"Gender\", \"Existing Customer\", \"State\",\n",
    "#         \"City\", \"Employment Profile\", \"Occupation\"\n",
    "#     ],\n",
    "#     \"numeric_columns\": [\n",
    "#         \"Age\", \"Income\", \"Credit Score\", \"Credit History Length\", \"Number of Existing Loans\",\n",
    "#         \"Loan Amount\", \"Loan Tenure\", \"LTV Ratio\"\n",
    "#     ],\n",
    "#     \"target_column\": \"Profile Score\",\n",
    "#     \"is_target_categorical\": False, \n",
    "# }\n",
    "\n",
    "# config = {\n",
    "#     # General\n",
    "#     \"training_session\": 5,\n",
    "\n",
    "#     # Mean-Teacher Model\n",
    "#     \"pre_trained\": True,\n",
    "#     \"learning_rate\": 3e-4,\n",
    "#     \"alpha\": 0.99,\n",
    "#     \"lambda_u\": 1.0,\n",
    "#     \"epochs\": 20,\n",
    "\n",
    "#     # Dataset\n",
    "#     \"input_type\": \"tabular\",\n",
    "#     \"dataset_path\": \"../../datasets/loan.csv\",\n",
    "#     \"num_labels\": 0.4,\n",
    "#     \"batch_size\": 64,\n",
    "\n",
    "#     # Image input\n",
    "#     \"image_classes\": [\"form\", \"invoice\", \"memo\", \"letter\"],\n",
    "#     \"image_size\": 224,\n",
    "\n",
    "#     # Text input\n",
    "#     \"text_column\": \"Sentence\",\n",
    "#     \"text_target_column\": \"Sentiment\",\n",
    "\n",
    "#     # Tabular input\n",
    "#     \"categorical_columns\": [\n",
    "#         \"ApplicationDate\", \"EmploymentStatus\", \"EducationLevel\",\n",
    "#         \"MaritalStatus\", \"HomeOwnershipStatus\", \"LoanPurpose\"\n",
    "#     ],\n",
    "#     \"numeric_columns\": [\n",
    "#         \"Age\", \"AnnualIncome\", \"CreditScore\", \"Experience\", \"LoanAmount\",\n",
    "#         \"LoanDuration\", \"NumberOfDependents\", \"MonthlyDebtPayments\",\n",
    "#         \"CreditCardUtilizationRate\", \"NumberOfOpenCreditLines\", \"NumberOfCreditInquiries\",\n",
    "#         \"DebtToIncomeRatio\", \"BankruptcyHistory\", \"PreviousLoanDefaults\", \"PaymentHistory\",\n",
    "#         \"LengthOfCreditHistory\", \"SavingsAccountBalance\", \"CheckingAccountBalance\",\n",
    "#         \"TotalAssets\", \"TotalLiabilities\", \"MonthlyIncome\", \"UtilityBillsPaymentHistory\",\n",
    "#         \"JobTenure\", \"NetWorth\", \"BaseInterestRate\", \"InterestRate\",\n",
    "#         \"MonthlyLoanPayment\", \"TotalDebtToIncomeRatio\", \"RiskScore\"\n",
    "#     ],\n",
    "#     \"target_column\": \"LoanApproved\",\n",
    "#     \"is_target_categorical\": True, \n",
    "# }\n",
    "\n",
    "config = {\n",
    "    # General\n",
    "    \"training_session\": 1,\n",
    "\n",
    "    # Mean-Teacher Model\n",
    "    \"pre_trained\": False,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"alpha\": 0.99,\n",
    "    \"lambda_u\": 1.0,\n",
    "    \"epochs\": 20,\n",
    "\n",
    "    # Dataset\n",
    "    \"input_type\": \"text\",\n",
    "    \"dataset_path\": \"../../datasets/sentiments.csv\",\n",
    "    \"num_labels\": 0.4,\n",
    "    \"batch_size\": 64,\n",
    "\n",
    "    # Image input\n",
    "    \"image_classes\": [],\n",
    "    \"image_size\": 224,\n",
    "\n",
    "    # Text input\n",
    "    \"text_column\": \"Sentence\",\n",
    "    \"text_target_column\": \"Sentiment\",\n",
    "\n",
    "    # Tabular input\n",
    "    \"categorical_columns\": [],\n",
    "    \"numeric_columns\": [],\n",
    "    \"target_column\": \"\",\n",
    "    \"is_target_categorical\": True, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "651b7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ema(student_model, teacher_model, alpha=0.99):\n",
    "    for student_param, teacher_param in zip(student_model.parameters(), teacher_model.parameters()):\n",
    "        teacher_param.data = alpha * teacher_param.data + (1 - alpha) * student_param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "75d350fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(student_model, teacher_model, lb_loader, ulb_loader, optimizer, device, epoch, alpha):\n",
    "    student_model.train()\n",
    "    teacher_model.train()\n",
    "\n",
    "    is_regression = True if config[\"input_type\"] == \"tabular\" and not config[\"is_target_categorical\"] else False\n",
    "    total_loss = 0\n",
    "    for batch_idx, ((x_lb, y_lb), (x_ulb_w, x_ulb_s)) in enumerate(zip(lb_loader, ulb_loader)):\n",
    "        print(f\"ðŸŸ¡ Batch {batch_idx}\")\n",
    "        \n",
    "        # TODO: Change\n",
    "        if config[\"input_type\"] == \"text\" and config[\"pre_trained\"]:\n",
    "            x_lb = {k: v.to(device) for k, v in x_lb.items()}\n",
    "            x_ulb_w = {k: v.to(device) for k, v in x_ulb_w.items()}\n",
    "            x_ulb_s = {k: v.to(device) for k, v in x_ulb_s.items()}\n",
    "        else:\n",
    "            print(x_ulb_w)\n",
    "            x_lb = x_lb.to(device)\n",
    "            x_ulb_w = x_ulb_w.to(device)\n",
    "            x_ulb_s = x_ulb_s.to(device)\n",
    "\n",
    "        y_lb = y_lb.to(device)\n",
    "\n",
    "        if is_regression:\n",
    "            y_lb = y_lb.float().unsqueeze(1).to(device)\n",
    "        else:\n",
    "            y_lb = y_lb.to(device)\n",
    "\n",
    "        # Supervised loss\n",
    "        logits_lb = student_model(x_lb)\n",
    "        loss_sup = F.mse_loss(logits_lb, y_lb) if is_regression else F.cross_entropy(logits_lb, y_lb)\n",
    "\n",
    "        # Unsupervised loss (consistency)\n",
    "        if is_regression:\n",
    "            with torch.no_grad():\n",
    "                pseudo_labels = teacher_model(x_ulb_w)\n",
    "            logits_ulb_s = student_model(x_ulb_s)\n",
    "            loss_unsup = F.mse_loss(logits_ulb_s, pseudo_labels)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                logits_ulb_w = teacher_model(x_ulb_w)\n",
    "                pseudo_labels = torch.softmax(logits_ulb_w, dim=1)\n",
    "            logits_ulb_s = student_model(x_ulb_s)\n",
    "            loss_unsup = F.mse_loss(torch.softmax(logits_ulb_s, dim=1), pseudo_labels)\n",
    "\n",
    "        # Total loss\n",
    "        loss = loss_sup + config[\"lambda_u\"] * loss_unsup\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # EMA update\n",
    "        update_ema(student_model, teacher_model, alpha)\n",
    "\n",
    "    print(f'Epoch {epoch} | Total Loss: {total_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "e89430b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    is_regression = True if config[\"input_type\"] == \"tabular\" and not config[\"is_target_categorical\"] else False\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            if config[\"input_type\"] == \"text\" and config[\"pre_trained\"]:\n",
    "                x = {k: v.to(device) for k, v in x.items()}\n",
    "            else:\n",
    "                x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "\n",
    "            if is_regression:\n",
    "                loss = F.mse_loss(logits.squeeze(), y.float())\n",
    "                preds = logits.squeeze()\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits, y.long())\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    if is_regression:\n",
    "        mae = np.mean(np.abs(np.array(all_preds) - np.array(all_labels)))\n",
    "        print(f\"Validation MAE: {mae:.4f} | Loss: {total_loss:.4f}\")\n",
    "        return mae, total_loss\n",
    "    else:\n",
    "        acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "        print(f\"Validation Accuracy: {acc:.4f} | Loss: {total_loss:.4f}\")\n",
    "        return acc, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e4464467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mean_teacher(student_model, lb_loader, ulb_loader, val_loader, device, epochs, alpha):\n",
    "    teacher_model = copy.deepcopy(student_model)\n",
    "    for param in teacher_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    optimizer = optim.Adam(student_model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    best_val_accuracy = 0\n",
    "    best_model_path = f\"../../models/mean_teacher/best_model_{config[\"input_type\"]}_{config[\"training_session\"]}.pt\"\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"ðŸŸ¡ Start of epoch {epoch}!\")\n",
    "        \n",
    "        train_one_epoch(student_model, teacher_model, lb_loader, ulb_loader, optimizer, device, epoch, alpha)\n",
    "\n",
    "        # TODO: Update logic to account for regression vs classification\n",
    "        val_accuracy, _ = evaluate(student_model, val_loader, device)\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(student_model.state_dict(), best_model_path)\n",
    "            print(f\"âœ… Best model saved to {best_model_path} | Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    return student_model, teacher_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "ecde0b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model_factory' from '/Users/dundale/Downloads/bpi-ssl/simulation/mean_teacher/model_factory.py'>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import token_factory as tf\n",
    "import dataloader_factory as dl\n",
    "import model_factory as md\n",
    "\n",
    "importlib.reload(tf)\n",
    "importlib.reload(dl)\n",
    "importlib.reload(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33112de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from token_factory import token_factory\n",
    "from model_factory import model_factory\n",
    "from dataloader_factory import dataloader_factory\n",
    "\n",
    "if config[\"input_type\"] == \"image\":\n",
    "    # Obtain the base transform for image inputs\n",
    "    base_transform = token_factory(\n",
    "        \"image\", \n",
    "        image_size=(config[\"image_size\"], config[\"image_size\"])\n",
    "    )\n",
    "    \n",
    "    # Create MLP model\n",
    "    model = model_factory(\n",
    "        \"image\", \n",
    "        num_classes=len(config[\"image_classes\"]), \n",
    "        pretrained=config[\"pre_trained\"]\n",
    "    ).to(device)\n",
    "\n",
    "    # Create dataloaders\n",
    "    lb_loader, ulb_loader, val_loader = dataloader_factory(\n",
    "        \"image\",\n",
    "        num_labels=config[\"num_labels\"],\n",
    "        image_classes=config[\"image_classes\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        dataset_path=config[\"dataset_path\"],\n",
    "        base_transform=base_transform\n",
    "    )\n",
    "\n",
    "    # Train Mean Teacher\n",
    "    trained_student, trained_teacher = train_mean_teacher(\n",
    "        model, lb_loader, ulb_loader, val_loader, device, config[\"epochs\"], config[\"alpha\"]\n",
    "    )\n",
    "\n",
    "elif config[\"input_type\"] == \"text\":\n",
    "    # Split dataset into labeled, unlabeled, and validation data\n",
    "    df = pd.read_csv(config[\"dataset_path\"])\n",
    "    unlabeled_size = 1.0 - config.get(\"num_labels\", 0.3)\n",
    "    df_labeled, df_unlabeled = train_test_split(df, test_size=unlabeled_size, stratify=df[config[\"text_target_column\"]], random_state=42)\n",
    "    df_train, df_val = train_test_split(df_labeled, test_size=0.2, stratify=df_labeled[config[\"text_target_column\"]])\n",
    "\n",
    "    # Instantiate tokenizer\n",
    "    tokenizer = token_factory(\n",
    "        \"text\",\n",
    "        text_column=config[\"text_column\"],\n",
    "        target_column=config[\"text_target_column\"],\n",
    "        pretrained=config[\"pre_trained\"],\n",
    "    )\n",
    "\n",
    "    # Fit only on labeled training data\n",
    "    tokenizer.fit(df_train)  \n",
    "\n",
    "    X_train = tokenizer.transform(df_train)\n",
    "    y_train = tokenizer.transform_target(df_train)\n",
    "\n",
    "    X_val = tokenizer.transform(df_val)\n",
    "    y_val = tokenizer.transform_target(df_val)\n",
    "\n",
    "    X_unlabeled = df_unlabeled[config[\"text_target_column\"]].tolist()\n",
    "\n",
    "    # Pass the underlying tensors to the dataloader_factory\n",
    "    lb_loader, val_loader, ulb_loader = dataloader_factory(\n",
    "        \"text\",\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        X_val=X_val, y_val=y_val,\n",
    "        X_unlabeled=X_unlabeled,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    # Determine number of classes\n",
    "    num_classes = len(np.unique(y_train.numpy())) \n",
    "    input_dim = X_train.shape[1] if not config[\"pre_trained\"] else None \n",
    "\n",
    "    # Instantiate model\n",
    "    model = model_factory(\n",
    "        \"text\",\n",
    "        num_classes=num_classes,\n",
    "        pretrained=config[\"pre_trained\"],\n",
    "        tfidf_dim=input_dim\n",
    "    ).to(device)\n",
    "\n",
    "    # Train Mean Teacher\n",
    "    trained_student, trained_teacher = train_mean_teacher(\n",
    "        model, lb_loader, ulb_loader, val_loader, device, config[\"epochs\"], alpha=config[\"alpha\"]\n",
    "    )\n",
    "\n",
    "elif config[\"input_type\"] == \"tabular\":\n",
    "    is_regression = not config[\"is_target_categorical\"]\n",
    "\n",
    "    # Split the dataset into labeled, unlabeled, and validation data\n",
    "    df = pd.read_csv(config[\"dataset_path\"])\n",
    "    unlabeled_size = 1.0 - config.get(\"num_labels\", 0.3)\n",
    "    df_labeled, df_unlabeled = train_test_split(df, test_size=unlabeled_size, stratify=df[config[\"target_column\"]], random_state=42)\n",
    "    df_train, df_val = train_test_split(df_labeled, test_size=0.2, stratify=df_labeled[config[\"target_column\"]])\n",
    "\n",
    "    # Obtain the tokenizer for tabular inputs and tokenize input\n",
    "    tokenizer = token_factory(\n",
    "        \"tabular\", \n",
    "        categorical_columns=config[\"categorical_columns\"],\n",
    "        numeric_columns=config[\"numeric_columns\"],\n",
    "        target_column=config[\"target_column\"],\n",
    "        is_target_categorical=config[\"is_target_categorical\"]\n",
    "    )\n",
    "\n",
    "    # Fit tokenizer on training data\n",
    "    tokenizer.fit(df_train)\n",
    "\n",
    "    # Tokenize features\n",
    "    X_train = tokenizer.transform(df_train)\n",
    "    y_train = tokenizer.transform_target(df_train)\n",
    "\n",
    "    X_val = tokenizer.transform(df_val)\n",
    "    y_val = tokenizer.transform_target(df_val)\n",
    "\n",
    "    X_unlabeled = tokenizer.transform(df_unlabeled)\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32 if not config[\"is_target_categorical\"] else torch.long)\n",
    "\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    if is_regression:\n",
    "        y_val = torch.tensor(y_val.to_numpy(), dtype=torch.float32 if not config[\"is_target_categorical\"] else torch.long)\n",
    "    else:\n",
    "        y_val = torch.tensor(y_val, dtype=torch.float32 if not config[\"is_target_categorical\"] else torch.long)\n",
    "\n",
    "    X_unlabeled = torch.tensor(X_unlabeled, dtype=torch.float32)\n",
    "\n",
    "    # Generate dataloaders\n",
    "    lb_loader, ulb_loader, val_loader = dataloader_factory(\n",
    "        \"tabular\", X_train=X_train, y_train=y_train, \n",
    "        X_val=X_val, y_val=y_val, X_unlabeled=X_unlabeled, \n",
    "        batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    input_dim = df.drop(columns=[config[\"target_column\"]]).shape[1]\n",
    "    num_classes = df[config[\"target_column\"]].nunique()\n",
    "    model = model_factory(\n",
    "        \"tabular\",\n",
    "        input_dim=input_dim,\n",
    "        num_classes=num_classes,\n",
    "        regression=is_regression\n",
    "    ).to(device)\n",
    "\n",
    "    # Train Mean Teacher\n",
    "    trained_student, trained_teacher = train_mean_teacher(\n",
    "        model, lb_loader, ulb_loader, val_loader, device, config[\"epochs\"], alpha=config[\"alpha\"]\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported input type: {config[\"input_type\"]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
